[cloudera@quickstart ~]$ pyspark --master local[4]

[cloudera@quickstart ~]$ vi number.txt

>>> from pyspark import SparkContext #declair sparkcontext

>>> sc.stop()

>>> from pyspark import SparkConf  #declair sparkcontext

>>> conf = SparkConf().setAppName("spark") # ("spark"), is a application name

>>> conf.getAll() # get configration
[(u'spark.app.name', u'spark'), (u'spark.master', u'local[4]'), (u'spark.submit.deployMode', u'client')]

>>> sc=SparkContext(conf=conf) # conf is declaired in {conf = SparkConf().setAppName("spark") # ("spark"), is a application name}

*1 way :rdd1
>>> rdd1=sc.textFile("file:///home/cloudera/number.txt")

>>> print(rdd1)
file://home/cloudera/number.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:-2

>>> rdd1.collect()
.
.
.
19/07/06 11:40:12 INFO scheduler.DAGScheduler: Job 0 finished: collect at <stdin>:1, took 1.535493 s
[u'one,two,three', u'one,two,three', u'one,two', u'three,two', u'one,one']

*2 way:rdd2

>>> rdd2=sc.parallelize([1,2,3,4,5,6,7,8,9])

>>> rdd2.collect()
.
.
.
19/07/06 11:43:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
[1, 2, 3, 4, 5, 6, 7, 8, 9]

*3 way:rdd3 {create rdd3 by rdd1} # rdd3 is referenced of rdd1

>>> rdd3=rdd1

>>> rdd3.collect()

.
.
19/07/06 11:51:28 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
[u'one,two,three', u'one,two,three', u'one,two', u'three,two', u'one,one']

*
>>> rdd4=sc.parallelize([1,2,3,4,5,6,7,8,9],10)

>>> rdd4.getNumPartitions()
10

*
Questions : how many numbers of way to create or assign, partition to the particular rdd. with examples
Q: why getNumPartitions not showing correct result.

>>> rdd5=sc.textFile("file:///home/cloudera/number.txt",15)

>>> rdd5.getNumPartitions()
19/07/06 11:57:06 INFO mapred.FileInputFormat: Total input paths to process : 1
18
